<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.554">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MeowNetWebpage - MeowNet: CatSound Classfication</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">MeowNetWebpage</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-question-we-answered" id="toc-what-question-we-answered" class="nav-link active" data-scroll-target="#what-question-we-answered"><span class="header-section-number">1</span> What Question We Answered</a></li>
  <li><a href="#what-prior-related-work-exists" id="toc-what-prior-related-work-exists" class="nav-link" data-scroll-target="#what-prior-related-work-exists"><span class="header-section-number">2</span> What Prior Related Work Exists</a></li>
  <li><a href="#how-our-approach-differs-from-prior-related-work" id="toc-how-our-approach-differs-from-prior-related-work" class="nav-link" data-scroll-target="#how-our-approach-differs-from-prior-related-work"><span class="header-section-number">3</span> How Our Approach Differs From Prior Related Work</a></li>
  <li><a href="#design-of-experiment" id="toc-design-of-experiment" class="nav-link" data-scroll-target="#design-of-experiment"><span class="header-section-number">4</span> Design of Experiment</a></li>
  <li><a href="#what-data-we-used-and-how-we-sourced-it" id="toc-what-data-we-used-and-how-we-sourced-it" class="nav-link" data-scroll-target="#what-data-we-used-and-how-we-sourced-it"><span class="header-section-number">5</span> What Data We Used and How We Sourced It</a></li>
  <li><a href="#neural-networks-we-trained" id="toc-neural-networks-we-trained" class="nav-link" data-scroll-target="#neural-networks-we-trained"><span class="header-section-number">6</span> Neural Networks We Trained</a></li>
  <li><a href="#how-we-measured-success" id="toc-how-we-measured-success" class="nav-link" data-scroll-target="#how-we-measured-success"><span class="header-section-number">7</span> How We Measured Success</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">8</span> Results</a></li>
  <li><a href="#future-work" id="toc-future-work" class="nav-link" data-scroll-target="#future-work"><span class="header-section-number">9</span> Future Work</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">10</span> References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="index.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MeowNet: CatSound Classfication</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div style="text-align:center;">
<p>Author: Ruiyang Chen, Yanxi Li, Haoze Wang</p>
</div>
<section id="what-question-we-answered" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="what-question-we-answered"><span class="header-section-number">1</span> What Question We Answered</h2>
<p>In our project, we explored the application of neural networks in recognizing cat sounds. We studied the performance of different neural networks on a dataset of cat meows, as well as the impact of transfer learning on recognition accuracy.</p>
</section>
<section id="what-prior-related-work-exists" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="what-prior-related-work-exists"><span class="header-section-number">2</span> What Prior Related Work Exists</h2>
<p>To explain the meaning of cat vocalizations, some methods based on deep learning have been used. For example, <span class="citation" data-cites="pandeya2018domestic">(<a href="#ref-pandeya2018domestic" role="doc-biblioref">Pandeya, Kim, and Lee 2018</a>)</span> employed transfer learning techniques to categorize cat vocalizations into ten classes, using a pre-trained neural network initially developed for music classification. In this way, they achieved 91.13% in accuracy. What’s more, a recent study explored the meanings conveyed by cat vocalizations in complex sound environments <span class="citation" data-cites="sun2021purrai">(<a href="#ref-sun2021purrai" role="doc-biblioref">Sun et al. 2021</a>)</span>. They built a very large cat vocalization database and categorized cat vocalizations into nine classes and utilized a modified convolutional neural network for cat vocalization recognition. And their sentence-based AI model achieved an accuracy of 81.1% for emotion prediction.</p>
<p>The attempt described in a study conducted by <span class="citation" data-cites="ntalampiras2013novel">(<a href="#ref-ntalampiras2013novel" role="doc-biblioref">S. Ntalampiras 2013</a>)</span> to recognize environmental sounds using HMMs demonstrated the classification of various environmental sounds into 15 categories. Among these 15 categories also included animal sounds (bird calls, dog barking, and cat meowing), and showed better sound recognition accuracy using universal modeling based on GMM clustering rather than simple HMM and balanced universal modeling.</p>
<p>Another study leveraged a modified YAMNet deep network, and proposed a novel pipeline for real-time detection and interpretation of individual cat intents from audio signals <span class="citation" data-cites="ntalampiras2021acoustic">(<a href="#ref-ntalampiras2021acoustic" role="doc-biblioref">Stavros Ntalampiras, Kosmin, and Sanchez 2021</a>)</span>. This approach addressed the inherent challenges posed by the non-stationarity of cat vocalizations and evolving environmental conditions, offering a framework that continuously updates its class dictionary based on user input. The reported average recognition rate of 90% under stationary conditions demonstrates the reliability of the model, with misclassifications primarily attributed to the high intra-class variability within the dataset.</p>
<p>A study discussed the use of a Convolutional Neural Network (CNN) to classify different sounds made by cats based on emotions <span class="citation" data-cites="ferdiana2021cat">(<a href="#ref-ferdiana2021cat" role="doc-biblioref">Ferdiana, Dicka, and Boediman 2021</a>)</span>. The study focused on using Mel-frequency cepstral coefficients (MFCCs) for feature extraction and training the model on a dataset of 595 cat sound samples categorized into five emotions: purring, meowing, hissing, caterwaul, and growling. The CNN architecture included four convolutional layers, a pooling layer, and a dense layer as the output layer. The model achieved a classification accuracy of 88.47%, with plans to improve accuracy by adding more data and implementing data augmentation techniques.</p>
</section>
<section id="how-our-approach-differs-from-prior-related-work" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="how-our-approach-differs-from-prior-related-work"><span class="header-section-number">3</span> How Our Approach Differs From Prior Related Work</h2>
<p>In our project, we didn’t solely rely on convolutional modules. Instead, we employed ResNet18, ResNet50, and EfficientNetV2-S to recognize cat sounds. We tested the impact of different modules contained in these networks on recognition accuracy, such as residual structures and MBConv modules. Additionally, due to the small size of our dataset, we utilized transfer learning. Our models were pretrained on the ImageNet dataset and then fine-tuned on the cat sound dataset.</p>
</section>
<section id="design-of-experiment" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="design-of-experiment"><span class="header-section-number">4</span> Design of Experiment</h2>
<p>Our experimental design includes several key steps to ensure a thorough evaluation of neural network models for cat sound classification. Firstly, we preprocess the audio data using the librosa library, applying techniques such as trimming, pre-emphasis, and utilizing mel-filters to get the filter bank features. Following preprocessing, we split the dataset into training, validation, and testing sets, maintaining class distribution across splits.</p>
<p>Next, we train multiple neural network models, including ResNet18, ResNet50, EfficientNetV2-S, and VGG13, alongside a custom SimpleNet architecture. We employ transfer learning by fine-tuning the pretrained models on the ImageNet dataset, utilizing appropriate loss functions like categorical cross-entropy and optimization algorithms such as Adam. Throughout the training process, we monitor metrics such as loss curves and accuracy plots to assess model performance.</p>
<p>After training, we evaluate the models on the validation set using metrics such as accuracy, precision, recall, and F1-score. This evaluation process helps us select the best-performing model based on validation set performance. We fine-tune hyperparameters if necessary to optimize model performance further. Finally, we assess the generalization performance of the selected model on the test set, generating a comprehensive report summarizing the experimental process and results.</p>
</section>
<section id="what-data-we-used-and-how-we-sourced-it" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="what-data-we-used-and-how-we-sourced-it"><span class="header-section-number">5</span> What Data We Used and How We Sourced It</h2>
<p>Our dataset comes from a paper (https://doi.org/10.3390/app8101949), and can be used for academic purposes with the author’s permission. This dataset consists of 10 categories (Angry, Defence, Fighting, Happy, HuntingMind, Mating, MotherCall, Paining, Resting, Warning). Each category consists of approximately 600 MP3 files, which are the original stereo audio and their corresponding processed mono audio.</p>
</section>
<section id="neural-networks-we-trained" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="neural-networks-we-trained"><span class="header-section-number">6</span> Neural Networks We Trained</h2>
<p>Our experiment involves training five different neural network architectures for cat sound classification. These include ResNet18, ResNet50, EfficientNetV2-S, VGG13, and a custom SimpleNet architecture. ResNet18 and ResNet50 are deep residual networks pretrained on the ImageNet dataset, whereas EfficientNetV2-S is an efficient convolutional neural network architecture also pretrained on ImageNet. Additionally, we design a custom SimpleNet architecture specifically tailored for the cat sound classification task.</p>
<p>ResNet18 and ResNet50 differ in their depth, with ResNet50 being deeper and potentially capturing more complex features. EfficientNetV2-S leverages efficient convolutional blocks to achieve better performance with fewer parameters. In contrast, VGG13 follows a simpler architecture design with repeated convolutional and max-pooling layers. SimpleNet, on the other hand, is a custom architecture designed to balance model complexity and performance. Each architecture undergoes fine-tuning on the cat sound dataset, enabling them to learn discriminative features for cat sound classification.</p>
</section>
<section id="how-we-measured-success" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="how-we-measured-success"><span class="header-section-number">7</span> How We Measured Success</h2>
<p>We use confusion matrices to evaluate the performance of our trained models on the validation set. For each network structure, we generate multiple confusion matrices, each corresponding to the model’s performance at a specific EPOCH. Through a series of images, we can clearly see how the performance of the model changes as the EPOCH value increases, following parameter adjustments. From these results, we can select the optimal model, which entails tuning the hyperparameter EPOCH value.</p>
<p>Additionally, we calculate metrics including accuracy, precision, and F1-score for each model we trained. Accuracy measures the overall correctness of predictions, while precision quantifies the proportion of true positive predictions among all positive predictions. And F1-score provides a balance between precision and recall. We found that, when the model is trained to a certain extent, that is, when there is neither underfitting nor overfitting, the values of these three metrics are quite close. We believe the reason for this occurrence is that the internal data division within our training and validation sets is very balanced, meaning the quantity of data for different targets is similar.</p>
<p>We plotted line graphs showing the changes in the three metrics over increasing EPOCH values during the training process of three different neural network architectures. Based on these graphs, we selected the optimal models from ResNet18, ResNet50, and EfficientNet. In the model selection process, we chose models corresponding to the part of the curves that were nearing or had reached stability, effectively avoiding the risks of underfitting and overfitting. Then, within this stable section of the curves, we selected points that performed well across all three metrics as our optimal models.</p>
</section>
<section id="results" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="results"><span class="header-section-number">8</span> Results</h2>
<p>By analyzing these metrics and observing the differences in the confusion matrices, we gained a deeper understanding of how different structured models performed under various EPOCH values (hyperparameters). We found that our chosen optimal model for ResNet18 performed as follows: Accuracy: 85.185%, Precision: 85.846%, F-1: 85.191%. This performance is very similar to, if not slightly better than, that of the optimal model for ResNet50, which showed an Accuracy of 84.637%, Precision: 85.012%, and F-1: 84.491%.</p>
<p>This is an interesting discovery, as although the ResNet50 structure is notably more complex than ResNet18, their performance on our Cat Sound Classification task was similar. This suggests that a more complex network structure does not necessarily yield better classification results, as complex neural networks may extract more irrelevant features and also carry a higher risk of overfitting. This highlights the importance of choosing an appropriate model structure for practical tasks, rather than just opting for more complex models. <br> <br></p>
<table class="table">
<thead>
<tr class="header">
<th>Models</th>
<th>Accuracy</th>
<th>Precision</th>
<th>F1-Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Resnet18</td>
<td>85.185%</td>
<td>85.846%</td>
<td>85.191%</td>
</tr>
<tr class="even">
<td>Resnet50</td>
<td>84.637%</td>
<td>85.012%</td>
<td>84.491%</td>
</tr>
<tr class="odd">
<td>EfficientnetV2_S</td>
<td>67.819%</td>
<td>69.852%</td>
<td>68.095%</td>
</tr>
</tbody>
</table>
<p><br> <br></p>
<p>The optimal model for EfficientNet performed significantly worse on our dataset, with Accuracy: 67.819%, Precision: 69.852%, and F-1: 68.095%. The model’s performance on the three evaluation metrics was about 16 percentage points lower than the two ResNet models. This indicates that the EfficientNet architecture is not very suitable for our cat sound classification dataset.</p>
<p>In conclusion, ResNet18 performed slightly better than ResNet50 in our task, and ResNet50 significantly outperformed EfficientNet.</p>
</section>
<section id="future-work" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="future-work"><span class="header-section-number">9</span> Future Work</h2>
<p>Moving forward, our focus will be on refining the proposed data augmentation methods to address challenges encountered in real-world sound reception scenarios. Environmental noise, interference, and sound overlap are common issues that can affect the performance of the sound classification model. By implementing additional data augmentation techniques, such as time stretching or pitch shifting, we aim to enhance the robustness of the model and ensure its effectiveness in practical applications.</p>
<p>Expanding the dataset with a wider variety of cat sound samples is another key aspect of our future work. By including more diverse samples, we can improve the model’s ability to generalize to different sound patterns and environments. Additionally, exploring advanced classification methods such as attention mechanisms or recurrent neural networks (RNNs) may offer further improvements in capturing temporal dependencies within cat sounds, enhancing the model’s overall performance.</p>
<p>Optimizing feature extraction methods such as Mel-frequency cepstral coefficients (MFCCs) or spectrogram representations will also be a focus of our future research. These methods play a crucial role in extracting discriminative features from cat sounds, and refining them could lead to significant enhancements in classification accuracy. Through these efforts, we aim to develop a sound classification system that meets the demands of real-world applications, including veterinary care and animal behavior research.</p>
</section>
<section id="references" class="level2" data-number="10">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">10 References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-ferdiana2021cat" class="csl-entry" role="listitem">
Ferdiana, R., W. F. Dicka, and A. Boediman. 2021. <span>“Cat Sounds Classification with Convolutional Neural Network.”</span> <em>International Journal on Electrical Engineering and Informatics</em> 13 (3): 755–65. <a href="https://doi.org/10.15676/ijeei.2021.13.3.15">https://doi.org/10.15676/ijeei.2021.13.3.15</a>.
</div>
<div id="ref-ntalampiras2013novel" class="csl-entry" role="listitem">
Ntalampiras, S. 2013. <span>“A Novel Holistic Modeling Approach for Generalized Sound Recognition.”</span> <em>IEEE Signal Processing Letters</em> 20 (2): 185–88. <a href="https://doi.org/10.1109/lsp.2013.2237902">https://doi.org/10.1109/lsp.2013.2237902</a>.
</div>
<div id="ref-ntalampiras2021acoustic" class="csl-entry" role="listitem">
Ntalampiras, Stavros, D. Kosmin, and J. Sanchez. 2021. <span>“Acoustic Classification of Individual Cat Vocalizations in Evolving Environments.”</span> In <em>2021 44th International Conference on Telecommunications and Signal Processing (TSP)</em>. <a href="https://doi.org/10.1109/tsp52935.2021.9522660">https://doi.org/10.1109/tsp52935.2021.9522660</a>.
</div>
<div id="ref-pandeya2018domestic" class="csl-entry" role="listitem">
Pandeya, Y. R., D. Kim, and J. Lee. 2018. <span>“Domestic Cat Sound Classification Using Learned Features from Deep Neural Nets.”</span> <em>Applied Sciences</em> 8 (10): 1949. <a href="https://doi.org/10.3390/app8101949">https://doi.org/10.3390/app8101949</a>.
</div>
<div id="ref-sun2021purrai" class="csl-entry" role="listitem">
Sun, W., V. Lu, A. Truong, H. Bossolina, and Y. Lu. 2021. <span>“Purrai: A Deep Neural Network Based Approach to Interpret Domestic Cat Language.”</span> In <em>2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)</em>. <a href="https://doi.org/10.1109/icmla52953.2021.00104">https://doi.org/10.1109/icmla52953.2021.00104</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
